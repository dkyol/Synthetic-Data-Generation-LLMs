{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932e3a28-f5b5-497d-be11-3f29417a01a2",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0127a4-11ad-4da8-98ad-169eb1fdd79e",
   "metadata": {},
   "source": [
    "# Responding in Your Own Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6b9a3-fa39-4396-9de2-31aea17cc00b",
   "metadata": {},
   "source": [
    "In this final notebook you will refactor your `respond_to_email` function to automatically reply to customer emails in a distinct voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3d3c6-e597-402d-9de9-8f03a9436b89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3750eac-1586-4595-88a2-71cda688a162",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542133cf-b013-4dcd-835a-a57a0694e9a1",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Learn about a tool to help you train models to respond in your own distinct voice\n",
    "- Be able to modify LLM responses to be in a distinct voice\n",
    "- Reflect on everything you've learned over the course of this workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765dec35-d94d-441a-beb2-28b08fb9719d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5409d57-d29b-47a4-ad64-9233d17760a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.helpers import collect_my_prompts_and_responses\n",
    "from llm_utils.models import LoraModels\n",
    "from llm_utils.postprocessors import strip\n",
    "from llm_utils.llm_functions import (\n",
    "    make_llm_function,\n",
    "    get_sentiment,\n",
    "    extract_name,\n",
    "    extract_product,\n",
    "    extract_location,\n",
    "    generate_customer_response_email\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e3194-44c2-4e32-9378-ea7551f1168a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e327e-b625-4ca3-9b24-c505db87a79e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c555d33-be7c-449f-acf4-c5bbcdf661cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt8b: gpt-8b-000-lora\n",
      "gpt43b: gpt-43b-002-lora\n"
     ]
    }
   ],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d509-006c-4e91-a5c2-07ad6b2adb36",
   "metadata": {},
   "source": [
    "## Load Customer Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d6d425-e35e-4dbe-af83-69d806d047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/solution_emails.json', 'r') as f:\n",
    "    customer_emails = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e916b-b9e4-402b-8c65-8f4cefd9c109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433d4f6-e21d-4f96-9942-4db29a6b2f63",
   "metadata": {},
   "source": [
    "# Responding in Your Own Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e292f6e-e053-4041-a419-6e4492e77a18",
   "metadata": {},
   "source": [
    "As a very last step in our task to autogenerate customer emails, we would like our email responses to capture our own voice and style of writing. A fantastic way to accomplish this is to fine-tune a model on text data that you have have written yourself in a natural setting.\n",
    "\n",
    "As we've learned, fine-tuning can work quite well with ~1000 samples, and sometimes less. With this in mind you can either sit down and write responses to several hundred to a thousand \"prompts\", which you could curate and/or synthetically generate yourself, or, you could make it a point as you go about your day having hundreds of natural text interactions with others to collect \"your prompts and responses\".\n",
    "\n",
    "The following opens a simple web page where you can submit \"your prompts and responses\" and saves them to `my_prompts_and_responses.jsonl` formatted correctly for NeMo Service p-tuning or LoRA.\n",
    "\n",
    "As always, be responsible and take care not to send data to 3rd party services when you should not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28df70e2-62b2-46ee-a8a1-a10d67183cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4723b85b04df46ab8c2bc99e65057d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='My Prompts and Responses'), Textarea(value='', description='Prompt:', layout=Layou‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collect_my_prompts_and_responses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478062b-0232-4388-a283-43635523b385",
   "metadata": {},
   "source": [
    "You could even easily imagine adding some additional logic that uploaded the data and kicked off a fine-tuning job once your data got to a size you were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85ab6ec-c70b-445f-abdd-04871b3af6bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1016a6e3-fe10-4604-be4e-244759887fda",
   "metadata": {},
   "source": [
    "## Responding With a Pirate's Voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ec611-0e23-4120-8a1a-864e53de9d1c",
   "metadata": {},
   "source": [
    "But since we don't have a bunch of data of your voice, and since if we provided you data of how we sounded you might not really be able to tell the difference, we've provided you with a model that has been LoRA fine-tuned on pirate responses. We call this, our final PEFT technique of the workshop, **P**irate-**tuning**. üè¥‚Äç‚ò†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa995ad4-94af-4586-825b-ae3a2db976ef",
   "metadata": {},
   "source": [
    "![Persona Create](images/persona_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f80a88-ad4c-46b0-a06a-f5e05910109d",
   "metadata": {},
   "source": [
    "Continuing to leverage the virtuous cycle we've been discussing throughout the day, we generated our synthetic pirate response data by 2-shot prompting GPT43B to repeat the emails generated in the previous notebook (though we generated 700 instead of 50, since we had the time) in the voice of a pirate.\n",
    "\n",
    "We passed the original (synthetically generated) emails into the following prompt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1827f115-c505-4474-903f-baecff2d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pirate_template(statement):\n",
    "    return f'Statement: {statement}Pirate Statement: '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f54b08-86c0-4e63-ba3e-57a2eb2a573b",
   "metadata": {},
   "source": [
    "...then saved GPT43B's responses as labels. We used the email/pirate-label pairs to perform LoRA for 1 epoch using GPT8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa75c75-54d9-4f9d-a988-d9a8b97a677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_model = NemoServiceBaseModel(LoraModels.gpt8b.value, customization_id='2d03bab3-0d79-44ff-b21f-bb6cea86b4fc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d62e93-186c-4e05-9747-4a91cbea8553",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44db00-a9c6-426b-a2c2-ebe5c61cd9b0",
   "metadata": {},
   "source": [
    "## Pirate LLM Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c7068-75a8-43fc-a46c-3ba6a17c4399",
   "metadata": {},
   "source": [
    "![Persona LLM Function](images/persona_llm_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a71155-833d-4cfe-9c08-5f084edbe767",
   "metadata": {},
   "source": [
    "Here we build a `pirate` LLM function using the LoRA fine-tuned GPT8B model and the `pirate_template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ec289b-ae3f-4598-b698-5806edb497b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate = make_llm_function(pirate_model, pirate_template, postprocessor=strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212d63e-d056-4ccf-b674-650273a81fbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca039da3-067e-4437-b86a-fd5c3264eeaf",
   "metadata": {},
   "source": [
    "## Try Pirate Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbbcdef-2bb7-44a6-a1e8-3bab875526b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"All of the sudden I know a lot about how to customize and orchestrate customized LLMs for fun and profit. \\\n",
    "Thank you prompt engineering. Thank you LoRA. Thank you NeMo.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7d5ad5-e839-4407-807d-c5247252d795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrrr, me hearty! I be havin' a grand time learnin' about how to customize and orchestrate customized LLMs for fun and profit, savvy? Thank ye, prompt engineering! Thank ye, LoRA! Thank ye, NeMo!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pirate(prompt, stop=['\\n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5cf40-d9d3-482e-9e88-555616840c7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e5688-96e3-495d-8ec4-5be90caa849a",
   "metadata": {},
   "source": [
    "## Auto Generate Response Emails in \"Your Own\" Voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdf7c04f-4f2f-4bca-8ead-14ca9bf23eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_email(email):\n",
    "    name = name_extractor(email)\n",
    "    sentiment = get_sentiment(email, tokens_to_generate=1)\n",
    "    product = product_extractor(email)\n",
    "    location = location_extractor(email)\n",
    "\n",
    "    response = write_response_email(company_name, name, sentiment, product, location)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf12143-8786-413b-98af-aa442ddc0c9d",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d5dfcde-bd81-4fdb-b6e1-6ef8e2c490e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_email(email):\n",
    "    name = extract_name(email)\n",
    "    sentiment = get_sentiment(email, tokens_to_generate=1)\n",
    "    product = extract_product(email)\n",
    "    location = extract_location(email)\n",
    "\n",
    "    company_name = 'StarBikes'\n",
    "    response = generate_customer_response_email(company_name, name, sentiment, product, location)\n",
    "    return pirate(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd714a2-c924-4b1d-97cc-eee2b25e08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "I bought a fat tire bike from StarBikes in Los Angeles. The bike has been a nightmare for me. I am a big guy and I need something that can handle my weight and the bike just isn't doing it. I am very disappointed with my purchase and I would like to get a full refund. Please contact me as soon as you can.\n",
      "Thanks,\n",
      "William\n",
      "\n",
      "\n",
      "Arrrr, me hearty!\n",
      "\n",
      "Ahoy, William!\n",
      "\n",
      "Thank ye for yer email, matey! We be sorry to hear about yer experience with yer fat tire bike, but we be happy to help ye out, savvy? Someone from our store in Los Angeles will be contacting ye soon to discuss the matter in more detail, and we'll do our best to make it right, me hearties!\n",
      "\n",
      "Fair winds and following seas,\n",
      "StarBikes Customer Support Team\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for customer_email in customer_emails[:1]:\n",
    "    print(customer_email+'\\n\\n')\n",
    "    print(respond_to_email(customer_email)+'\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd8bb3-38ff-4999-87ba-2ded8c68a853",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
