{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f7516-57f9-471f-8bd9-b080127a688b",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da992d2-2293-4519-8ebb-5a856db69642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb453e-7613-4dc9-9715-087799586a53",
   "metadata": {},
   "source": [
    "In this notebook you will begin work on an extractive question answering task using the [Stanford Question Answering](https://rajpurkar.github.io/SQuAD-explorer/) (SQuAD) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f27af-228b-4e0e-b718-8345cb50f511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdf224-2849-43af-9265-e33d221b10b7",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e18c9-1f9e-4f67-8ad8-f5082c95b021",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Be familiar with the SQuAD question answering dataset.\n",
    "- Observe zero-shot performance for extractive question answering using GPT43B and GPT8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ec8-042a-456b-bdc9-457739a75344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ce63-7179-405b-86cf-03e49f51f354",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1c11-09be-40f1-aceb-09535d298f0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c7df4-40ae-40dd-826c-93380de3c175",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babb9dd1-f059-4397-b4c9-6b3a37263fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt8b: gpt-8b-000\n",
      "gpt20b: gpt20b\n",
      "gpt43b_2: gpt-43b-002\n",
      "gpt43b: gpt-43b-001\n",
      "llama70b_chat: llama-2-70b-chat-hf\n",
      "llama70b: llama-2-70b-hf\n"
     ]
    }
   ],
   "source": [
    "Models.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d25de-1012-4c43-9581-299bfb3aacd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017a817-edab-4029-abb1-4ed21b094ffb",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe245c-bdfc-4f32-9d32-1f4c6dd72f51",
   "metadata": {},
   "source": [
    "For the question answering task, we will be working with the Stanford Question Answering Dataset (SQuAD). From the SQuAD documentation:\n",
    "\n",
    "> SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "The dataset contains over 100,000 questions and either its answer, or, that the question in unanswerable from the provided textual context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2dc1065-234a-4478-9ae4-0ab82ff11058",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad.json', 'r') as f:\n",
    "    squad_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39352c75-eb4b-477e-81bd-1ab701cd1f0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4eaa56-6967-41e2-949c-38b494372865",
   "metadata": {},
   "source": [
    "## Explore SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1424e-988e-4cc3-a1d2-79c1e8546b0c",
   "metadata": {},
   "source": [
    "The dataset comes as a dictionary with only 2 keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141479ac-bef7-4a0d-ad9e-b87914c36612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'data'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdb4a6-9fe1-4986-b200-73c6b5ee8a26",
   "metadata": {},
   "source": [
    "We are entirely interested in `data` which contains 442 different topics, each with many textual contexts and then questions and answers based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30e841a-1c54-4a30-8afd-d4ba096fc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = squad_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748ad181-70b7-4084-8987-f64a5e895f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da2c06c-d831-4484-af05-7c3c2c541519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Beyoncé\n",
      "Topic: Frédéric_Chopin\n",
      "Topic: Sino-Tibetan_relations_during_the_Ming_dynasty\n",
      "Topic: IPod\n",
      "Topic: The_Legend_of_Zelda:_Twilight_Princess\n",
      "Topic: Spectre_(2015_film)\n",
      "Topic: 2008_Sichuan_earthquake\n",
      "Topic: New_York_City\n",
      "Topic: To_Kill_a_Mockingbird\n",
      "Topic: Solar_energy\n"
     ]
    }
   ],
   "source": [
    "for d in data[:10]:\n",
    "    print(f'Topic: {d['title']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb825456-c7e9-4b90-9b8d-23e049121245",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bdead-f9a3-4e95-b9d1-88899da0df2b",
   "metadata": {},
   "source": [
    "## Explore Beyoncé Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fc804-5cb8-4c8d-90ff-9ebcb9389a5a",
   "metadata": {},
   "source": [
    "Let's take a look at the first topic in the dataset, which is about the pop singer Beyoncé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f8cafd-ee54-4b2a-99a3-643f8d2b2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "beyonce = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e0955e-5d64-41d1-a11e-de7d1cb0ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'paragraphs'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beyonce.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2677745-be2c-4fa0-a576-073bc58e0de4",
   "metadata": {},
   "source": [
    "Each topic contains a collection of context paragraphs that serve as the basis for the question answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b208e554-14ef-473f-ad1a-64edd2c49ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = beyonce['paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec38606-ab17-408e-8284-d88261e552e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950385dd-6f3b-4607-86bc-3291940335b9",
   "metadata": {},
   "source": [
    "In the case of the Beyoncé topic we can see that there are 66 context paragraphs, each with their own set of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da968b69-3176-4408-b601-2cba66c94250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb53932-30e1-4962-9869-a421ce1fc7a0",
   "metadata": {},
   "source": [
    "### Context, Questions and Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccb417-e07b-4320-8ce4-8b082f43f6fb",
   "metadata": {},
   "source": [
    "Let's look at the first contextual paragraph and its questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7088814b-5243-4624-b171-838071641f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a90dc91f-d9d8-4446-ac3f-7544bef91b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qas', 'context'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f1b71c-2969-4a73-a829-de9fc775f480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b37ec3-bc20-42e8-b7ee-0c74575ef477",
   "metadata": {},
   "source": [
    "This particular contextual paragraph has 15 question/answer pairs associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2966ab-90bf-4422-bb90-d47f7454c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qas = paragraph['qas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556b092b-0259-4060-9a35-f1c8d47f32e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d4784-1a44-43a5-ab16-91101ee7382b",
   "metadata": {},
   "source": [
    "Here's the structure of a single question/answer pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ca62a2a-fbb3-4771-9a06-5d971dd14ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When did Beyonce start becoming popular?',\n",
       " 'id': '56be85543aeaaa14008c9063',\n",
       " 'answers': [{'text': 'in the late 1990s', 'answer_start': 269}],\n",
       " 'is_impossible': False}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f14496-5eb1-4f49-95ad-7b9793434d32",
   "metadata": {},
   "source": [
    "Let's take a look at a few of the questions and their answers, and confirm that the answers are derived from text in the provided context paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9243f862-d1e8-4e13-b761-5a2d372f9c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: in the late 1990s\n",
      "Answer in paragraph: True\n",
      "\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: singing and dancing\n",
      "Answer in paragraph: True\n",
      "\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: 2003\n",
      "Answer in paragraph: True\n",
      "\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: Houston, Texas\n",
      "Answer in paragraph: True\n",
      "\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: late 1990s\n",
      "Answer in paragraph: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qa in qas[:5]:\n",
    "    question = qa['question']\n",
    "    answer = qa['answers'][0]['text']\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Answer: {answer}')\n",
    "    print(f'Answer in paragraph: {answer in paragraph['context']}\\n') # See `paragraph['context']` above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa6a99-9d99-43f5-8917-22a7487af193",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e3a87-f628-4b1f-9a81-fdb33da4d91e",
   "metadata": {},
   "source": [
    "## Process SQuAD Data Into Context, Question, Answer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48882ad4-45bb-441c-93ba-0b080bab881e",
   "metadata": {},
   "source": [
    "Ultimately we are going to use SQuAD data to fine tune a model on a question answering task. To that end it will be helpful to process the SQuAD data to simplify its structure and create a list where each item contains a context, question, and answer.\n",
    "\n",
    "Knowing what we do about the structure of the SQuAD data above we can run the following cell to do just this.\n",
    "\n",
    "Note that SQuAD contains some questions that are intentionally impossible to answer based on the provided context. We are going to choose to ignore these questions and instead only use those that have a clear answer.\n",
    "\n",
    "Also remember that SQuAD contains over 100,000 questions and answers. We know that for PEFT we can typically do well with roughly 1000 samples. With that in mind, and to keep our dataset diverse, we are only going to take the first context paragraph and its questions and answers for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d3297d-0ffe-456d-b0cb-77e5af0cc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_questions_answers = []\n",
    "for topic in data:\n",
    "    cqa = topic['paragraphs'][0]\n",
    "    context = cqa['context']\n",
    "    for qa in cqa['qas']:\n",
    "        if qa['is_impossible']:\n",
    "            continue\n",
    "        question = qa['question']\n",
    "        answer = qa['answers'][0]['text']\n",
    "        contexts_questions_answers.append({'context': context, 'question': question, 'answer': answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85becaa2-d6b4-4938-bc70-fe091238e629",
   "metadata": {},
   "source": [
    "This leaves us with over 2000 context, question, answer items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02d6d40b-e7a3-4a80-9403-2bbef9cb191f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts_questions_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f402db08-e107-4250-8799-f3a4997e3132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       "  'question': 'When did Beyonce start becoming popular?',\n",
       "  'answer': 'in the late 1990s'},\n",
       " {'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       "  'question': 'What areas did Beyonce compete in when she was growing up?',\n",
       "  'answer': 'singing and dancing'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts_questions_answers[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b26799-3ee4-4df2-8495-0cbb430f5657",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439a5df-75a4-4d7e-a95a-0ad7661ba6be",
   "metadata": {},
   "source": [
    "## Shuffle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b623c-7a06-4c1a-b7e4-adf2c002c44c",
   "metadata": {},
   "source": [
    "Even though we only took the first context paragraph for each topic in the dataset, we still have many questions for each of those context paragraphs. With that in mind, let's shuffle the data.\n",
    "\n",
    "We set a random seed here for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c0e039c-339b-4980-9117-2edf4150c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f0139f3-5f8e-4ee2-99ce-a6e56dcb9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(contexts_questions_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc1c32e4-8e19-4479-b054-dfd0e286e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The National Archives and Records Administration (NARA) is an independent agency of the United States government charged with preserving and documenting government and historical records and with increasing public access to those documents, which comprise the National Archives. NARA is officially responsible for maintaining and publishing the legally authentic and authoritative copies of acts of Congress, presidential proclamations and executive orders, and federal regulations. The NARA also transmits votes of the Electoral College to Congress.\n",
      "\n",
      "NARA is responsible for what collection of archives?\n",
      "National Archives\n",
      "-----\n",
      "\n",
      "Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene Epipaleolithic period and commenced with the beginning of farming, which produced the \"Neolithic Revolution\". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n",
      "\n",
      "What era is commonly known as the final phase of the Stone Age?\n",
      "the Neolithic\n",
      "-----\n",
      "\n",
      "In the Pre-Modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.\n",
      "\n",
      "What were Religious officials perceived as in the Pre-Modern era?\n",
      "spiritual intermediaries\n",
      "-----\n",
      "\n",
      "The dissolution of the Soviet Union was formally enacted on December 26, 1991, as a result of the declaration no. 142-Н of the Soviet of the Republics of the Supreme Soviet of the Soviet Union. The declaration acknowledged the independence of the former Soviet republics and created the Commonwealth of Independent States (CIS), although five of the signatories ratified it much later or not at all. On the previous day, Soviet President Mikhail Gorbachev, the eighth and last leader of the Soviet Union, resigned, declared his office extinct, and handed over its powers – including control of the Soviet nuclear missile launching codes – to Russian President Boris Yeltsin. That evening at 7:32 p.m., the Soviet flag was lowered from the Kremlin for the last time and replaced with the pre-revolutionary Russian flag.\n",
      "\n",
      "Who was president of the Soviet Union when it came to an end?\n",
      "Mikhail Gorbachev,\n",
      "-----\n",
      "\n",
      "USB was designed to standardize the connection of computer peripherals (including keyboards, pointing devices, digital cameras, printers, portable media players, disk drives and network adapters) to personal computers, both to communicate and to supply electric power. It has become commonplace on other devices, such as smartphones, PDAs and video game consoles. USB has effectively replaced a variety of earlier interfaces, such as serial and parallel ports, as well as separate power chargers for portable devices.\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals?\n",
      "USB\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cqa in contexts_questions_answers[:5]:\n",
    "    print(cqa['context']+'\\n')\n",
    "    print(cqa['question'])\n",
    "    print(cqa['answer']+'\\n-----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50518039-cf91-49b9-901f-ca76b8cf5abe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcd383-3f3b-4ea2-b24f-fb0747d7def7",
   "metadata": {},
   "source": [
    "## Question Answering Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00284c-6e62-4c0a-a33b-e891252e95b7",
   "metadata": {},
   "source": [
    "We will continue the practice of denoting our LLM tasks with a prompt template function. In the case of extractive question answering, we will use the following, which constructs a prompt given a provided `text` context and the `question` we would like answered from the provided `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1531cac-4bc6-447e-821e-0709beda44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_template(text, question):\n",
    "    return f'{text}\\n{question} answer: '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102c0fb-0738-4985-9234-3bf783819c78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263b419d-fe31-4a87-bb86-4715a7b3936c",
   "metadata": {},
   "source": [
    "## Create Prompts with Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df4950-e47c-4ffd-81b2-9fd5464a8b77",
   "metadata": {},
   "source": [
    "Now we can combine our `contexts_questions_answers` with the `extract_template` to create a list of prompts and their labels, which we will be able to leverage when working with our LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06f343c-4cd0-47da-8bc6-2fd20360f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = []\n",
    "for cqa in contexts_questions_answers:\n",
    "    context, question, answer = cqa['context'], cqa['question'], cqa['answer']\n",
    "    prompt = extract_template(context, question)\n",
    "    prompts_and_answers.append((prompt, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e06adae-e23b-4959-a997-a5882a4811d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a75b0966-96dc-482d-9db2-041c81401f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The National Archives and Records Administration (NARA) is an independent agency of the United States government charged with preserving and documenting government and historical records and with increasing public access to those documents, which comprise the National Archives. NARA is officially responsible for maintaining and publishing the legally authentic and authoritative copies of acts of Congress, presidential proclamations and executive orders, and federal regulations. The NARA also transmits votes of the Electoral College to Congress.\n",
      "NARA is responsible for what collection of archives? answer: \n",
      "\n",
      "National Archives\n",
      "---\n",
      "\n",
      "Traditionally considered the last part of the Stone Age, the Neolithic followed the terminal Holocene Epipaleolithic period and commenced with the beginning of farming, which produced the \"Neolithic Revolution\". It ended when metal tools became widespread (in the Copper Age or Bronze Age; or, in some geographical regions, in the Iron Age). The Neolithic is a progression of behavioral and cultural characteristics and changes, including the use of wild and domestic crops and of domesticated animals.\n",
      "What era is commonly known as the final phase of the Stone Age? answer: \n",
      "\n",
      "the Neolithic\n",
      "---\n",
      "\n",
      "In the Pre-Modern era, many people's sense of self and purpose was often expressed via a faith in some form of deity, be that in a single God or in many gods. Pre-modern cultures have not been thought of creating a sense of distinct individuality, though. Religious officials, who often held positions of power, were the spiritual intermediaries to the common person. It was only through these intermediaries that the general masses had access to the divine. Tradition was sacred to ancient cultures and was unchanging and the social order of ceremony and morals in a culture could be strictly enforced.\n",
      "What were Religious officials perceived as in the Pre-Modern era? answer: \n",
      "\n",
      "spiritual intermediaries\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[0:3]:\n",
    "    print(prompt+'\\n')\n",
    "    print(answer+'\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea93c88-30ef-41e3-823a-edb00cfa438e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3504da-258e-4964-942b-63531ef537b7",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting with GPT43B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df6a9bf-2021-4233-83cd-fac70cf52ba3",
   "metadata": {},
   "source": [
    "Let's see how GPT43B performs on this extractive question answering task with straightforward zero-shot prompting. First we'll instantiate an instance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13379b59-6e19-43d3-bf25-86fefeb88652",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = NemoServiceBaseModel(Models.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11510a1a-ee81-44c5-a3bc-1f2e7e0f7045",
   "metadata": {},
   "source": [
    "Next we'll try it out on the first several prompts in `prompts_and_answers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "070d88ad-a1ce-4d60-b6a1-da4d7309f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: National Archives\n",
      "Answer: National Archives\n",
      "\n",
      "Response: Neolithic\n",
      "Answer: the Neolithic\n",
      "\n",
      "Response: spiritual intermediaries\n",
      "Answer: spiritual intermediaries\n",
      "\n",
      "Response: Mikhail Gorbachev\n",
      "Answer: Mikhail Gorbachev,\n",
      "\n",
      "Response: USB\n",
      "Answer: USB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt43b.generate(prompt).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58588073-ddc2-4ec2-8760-038baf1a6810",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e891a5b-44b1-482e-aba1-c865328959bf",
   "metadata": {},
   "source": [
    "At a glance, it looks like GPT43B is well suited for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c06c17-3464-4c51-90a4-4aa853dbfa37",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3946a9-89eb-4533-bd95-0b1430cc22ab",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting with GPT8B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde9aa9-2066-4720-b0f9-07b4f5214df5",
   "metadata": {},
   "source": [
    "Now let's see how the much smaller GPT8B does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4336f0f4-92dd-428d-8f0b-a9c199a29e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt8b = NemoServiceBaseModel(Models.gpt8b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "506ef396-2c39-49f2-9081-c2d937875ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1. National Archives and Records Administration (NARA) 2. National Archives of the United States (NAUS) 3. National Archives of the United States (NAUS) 4. National Archives of the United States (NAUS) 5. National Archives of the United States (NAUS)\n",
      "\n",
      "What is the National Archives and Records Administration (NARA)?\n",
      "\n",
      "The National Archives and Records Administration (NARA) is an independent agency of the United States government charged with preserving and documenting government and historical records and with increasing public access to those documents, which comprise the National Archives. NARA is officially responsible for maintaining and publishing the legally authentic and authoritative copies of acts of Congress, presidential proclamations and executive orders, and federal regulations. The NARA also transmits votes of the Electoral College to Congress.\n",
      "\n",
      "What is the National Archives and Records Administration (NARA)?\n",
      "\n",
      "The National Archives and Records Administration (NARA) is an independent agency of the United States\n",
      "Answer: National Archives\n",
      "\n",
      "Response: 1. The Neolithic period is the final phase of the Stone Age. 2. The Neolithic period is the final phase of the Stone Age. 3. The Neolithic period is the final phase of the Stone Age. 4. The Neolithic period is the final phase of the Stone Age. 5. The Neolithic period is the final phase of the Stone Age.\n",
      "What is the final phase of the Stone Age? answer: 1. The Neolithic period is the final phase of the Stone Age. 2. The Neolithic period is the final phase of the Stone Age. 3. The Neolithic period is the final phase of the Stone Age. 4. The Neolithic period is the final phase of the Stone Age. 5. The Neolithic period is the final phase of the Stone Age.\n",
      "What is the final phase of the Stone Age? answer: 1. The Neolithic period is the final phase of the Stone Age. 2. The Neolithic period is the final phase\n",
      "Answer: the Neolithic\n",
      "\n",
      "Response: 1. Religious officials were perceived as the intermediaries between the common people and the divine. 2. Religious officials were perceived as the keepers of tradition. 3. Religious officials were perceived as the enforcers of social order.\n",
      "What were the roles of Religious officials in the Pre-Modern era? answer: 1. Religious officials were the intermediaries between the common people and the divine. 2. Religious officials were the keepers of tradition. 3. Religious officials were the enforcers of social order.\n",
      "What were the roles of Religious officials in the Modern era? answer: 1. Religious officials were the intermediaries between the common people and the divine. 2. Religious officials were the keepers of tradition. 3. Religious officials were the enforcers of social order.\n",
      "What were the roles of Religious officials in the Post-Modern era? answer: 1. Religious officials were the intermediaries between the common people and the divine. 2. Religious officials were the keepers of tradition\n",
      "Answer: spiritual intermediaries\n",
      "\n",
      "Response: 1991-1992: Boris Yeltsin. 1992-1993: Boris Yeltsin. 1993-1994: Boris Yeltsin. 1994-1995: Boris Yeltsin. 1995-1996: Boris Yeltsin. 1996-1997: Boris Yeltsin. 1997-1998: Boris Yeltsin. 1998-1999: Boris Yeltsin. 1999-2000: Boris Yeltsin. 2000-2001: Boris Yeltsin. 2001-2002: Boris Yeltsin. 2002-2003: Boris Yeltsin. 2003-2004: Boris Yeltsin. 2004-\n",
      "Answer: Mikhail Gorbachev,\n",
      "\n",
      "Response: 1. USB 2. Serial 3. Parallel 4. Firewire 5. Bluetooth\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals?\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals? answer: 1. USB 2. Serial 3. Parallel 4. Firewire 5. Bluetooth\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals?\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals? answer: 1. USB 2. Serial 3. Parallel 4. Firewire 5. Bluetooth\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals?\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals? answer: 1. USB 2. Serial 3. Parallel 4. Firewire 5. Bluetooth\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals?\n",
      "\n",
      "What was designed to standardize the connection of computer peripherals? answer: 1. USB\n",
      "Answer: USB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt8b.generate(prompt).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba3f08-484b-4852-82b0-cf58eb827d17",
   "metadata": {},
   "source": [
    "At the least, GPT8B seems to be going on and on, let's try again, indicating that we would like the model to stop generating after newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dbf13ff-b836-4b16-a694-e32a677f464e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1. National Archives and Records Administration (NARA) 2. National Archives of the United States (NAUS) 3. National Archives of the United States (NAUS) 4. National Archives of the United States (NAUS) 5. National Archives of the United States (NAUS)\n",
      "Answer: National Archives\n",
      "\n",
      "Response: 1. The Neolithic period is the final phase of the Stone Age. 2. The Neolithic period is the final phase of the Stone Age. 3. The Neolithic period is the final phase of the Stone Age. 4. The Neolithic period is the final phase of the Stone Age. 5. The Neolithic period is the final phase of the Stone Age.\n",
      "Answer: the Neolithic\n",
      "\n",
      "Response: 1. Religious officials were perceived as the intermediaries between the common people and the divine. 2. Religious officials were perceived as the keepers of tradition. 3. Religious officials were perceived as the enforcers of social order.\n",
      "Answer: spiritual intermediaries\n",
      "\n",
      "Response: 1991-1992: Boris Yeltsin. 1992-1993: Boris Yeltsin. 1993-1994: Boris Yeltsin. 1994-1995: Boris Yeltsin. 1995-1996: Boris Yeltsin. 1996-1997: Boris Yeltsin. 1997-1998: Boris Yeltsin. 1998-1999: Boris Yeltsin. 1999-2000: Boris Yeltsin. 2000-2001: Boris Yeltsin. 2001-2002: Boris Yeltsin. 2002-2003: Boris Yeltsin. 2003-2004: Boris Yeltsin. 2004-\n",
      "Answer: Mikhail Gorbachev,\n",
      "\n",
      "Response: 1. USB 2. Serial 3. Parallel 4. Firewire 5. Bluetooth\n",
      "Answer: USB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[:5]:\n",
    "    response = gpt8b.generate(prompt, stop=['\\n']).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Answer: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b454a1-8f8d-43d5-b5e2-6471d8579b3c",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a209e6-78fc-40e2-9eb0-a20fb81c6988",
   "metadata": {},
   "source": [
    "GPT8B continues to generate much more than we would like. It often repeats itself. It does not appear to be providing an answer extracted from the provided context. It is sometimes (see \"Boris Yeltsin\") wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0ebb5-a999-4269-968a-74e7a0120df8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1ceba-a447-4741-97ef-53230d7ff2f6",
   "metadata": {},
   "source": [
    "## Write Prompts and Answers to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42581925-87b0-472c-9656-5c27ad306b33",
   "metadata": {},
   "source": [
    "In the next section we will turn our attention to fine-tuning GPT8B on this task and it will be helpful to reuse the `prompts_and_answers` list that we created here. Let's write it to file so we can easily load it into the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79a7c771-c24d-4004-9b9b-3fe9a985c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad_prompts_and_answers.json', 'w') as f:\n",
    "    json.dump(prompts_and_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86280b3d-fb4d-4d73-81dd-ac92da56457f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
