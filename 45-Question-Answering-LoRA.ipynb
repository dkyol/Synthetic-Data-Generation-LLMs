{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8f7516-57f9-471f-8bd9-b080127a688b",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da992d2-2293-4519-8ebb-5a856db69642",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LoRA for Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb453e-7613-4dc9-9715-087799586a53",
   "metadata": {},
   "source": [
    "In this notebook you will fine tune GPT8B with LoRA to perform extractive question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125cd9b-3dda-44a8-bd20-a4f7adcaf7fb",
   "metadata": {},
   "source": [
    "![Extract LoRA](images/extract_lora.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f27af-228b-4e0e-b718-8345cb50f511",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acdf224-2849-43af-9265-e33d221b10b7",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e18c9-1f9e-4f67-8ad8-f5082c95b021",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "- LoRA fine tune a GPT8B model for extractive question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084ec8-042a-456b-bdc9-457739a75344",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ce63-7179-405b-86cf-03e49f51f354",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2807cfa5-a29a-4e8b-ac92-063655341429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llm_utils.models import LoraModels, Models\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.mocks import upload_qa as upload\n",
    "from llm_utils.mocks import create_qa_lora_customization as create_customization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b1c11-09be-40f1-aceb-09535d298f0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c7df4-40ae-40dd-826c-93380de3c175",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14eaeea-5961-4bc1-87fd-90fa9bbc2749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt8b: gpt-8b-000-lora\n",
      "gpt43b: gpt-43b-002-lora\n"
     ]
    }
   ],
   "source": [
    "LoraModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d25de-1012-4c43-9581-299bfb3aacd8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017a817-edab-4029-abb1-4ed21b094ffb",
   "metadata": {},
   "source": [
    "## Load Train Data From File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa695035-c087-4949-bd10-f7e3c3ac5ed4",
   "metadata": {},
   "source": [
    "We will begin this notebook by loading the train and test prompt and label data we created in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f847fdf-d8d1-4fa9-a1c0-370d489dbba9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365edb41-6558-4c95-8d51-c446b2ab1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/squad_prompts_and_answers.json', 'r') as f:\n",
    "    prompts_and_answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b0a4e3-3b94-4cbd-bcb1-430c136b5872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2349"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d783aa1-715b-4131-a0a5-8aed17c77e6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef7d73-4f92-4050-9410-c72be9d80a41",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7425c3-957f-4843-9854-1776b0571933",
   "metadata": {},
   "source": [
    "In preparation for fine-tuning, let's split the data, which currently contains over 2000 samples. We'll create a training set of 1000 samples, a validation set of 200 samples, and a small test set of 20 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f6a405-14d1-4803-84d9-6b24afbaeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = 1000\n",
    "val_n = 200\n",
    "test_n = 20\n",
    "\n",
    "train_end = train_n\n",
    "val_end = train_end + val_n\n",
    "test_end = val_end + test_n\n",
    "\n",
    "train_prompts_and_answers = prompts_and_answers[:train_n]\n",
    "val_prompts_and_answers = prompts_and_answers[train_n: train_n+val_n]\n",
    "test_prompts_and_answers = prompts_and_answers[train_n+val_n: train_n+val_n+test_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97147429-dc7c-4f03-bf0b-103f03886445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146762a2-ee12-40c6-af78-ef98f2a6ecf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7868605-a8f6-45fa-9333-135f88133fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_prompts_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86153b-e9f0-48cb-8d25-e9b43dcf7701",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d32ab3-1ef2-40da-87da-3b8347ef93f1",
   "metadata": {},
   "source": [
    "## Exercise: Format Data Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f11996-0036-4dc3-bb02-2c5438c5e4d8",
   "metadata": {},
   "source": [
    "For this exercise, you will format `train_prompts_and_answers` and `val_prompts_and_answers` for NeMo Service fine tuning.\n",
    "\n",
    "As a reminder, NeMo Service expects that data be in JSON Lines (`jsonl`) format, with each line in the file being in the following format:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87165f3f-c21b-42f0-bb96-3605bb2e7224",
   "metadata": {},
   "source": [
    "```python\n",
    "{\"prompt\": <prompt>, \"completion\": <completion/label>}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f504a1-07e4-4b87-8ee3-48d5fdfb677f",
   "metadata": {},
   "source": [
    "Your task is to populate the `qa_lora_train_data` and `qa_lora_val_data` lists with one dictionary for each data sample in `train_prompts_and_answers` and `val_prompts_and_answers` respectively, formatted as needed for NeMo Service LoRA fine-tuning.\n",
    "\n",
    "If you get stuck, feel free to look at the solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b69bf80-265a-4716-91be-062b4ed70dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_lora_train_data = [{'prompt': prompt, 'completion': answer} for prompt, answer in train_prompts_and_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f04a7aef-d88d-404b-8a11-943b2b3b1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_lora_val_data = [{'prompt': prompt, 'completion': answer} for prompt, answer in val_prompts_and_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80be0a-6c9d-446f-acd1-db1fd4500d50",
   "metadata": {},
   "source": [
    "Here we see examples of data well-formatted for p-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5e779f-c264-41fc-95f1-94cd1b4488ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The National Archives and Records Administration (NARA) is an independent agency of the United States government charged with preserving and documenting government and historical records and with increasing public access to those documents, which comprise the National Archives. NARA is officially responsible for maintaining and publishing the legally authentic and authoritative copies of acts of Congress, presidential proclamations and executive orders, and federal regulations. The NARA also transmits votes of the Electoral College to Congress.\\nNARA is responsible for what collection of archives? answer: ',\n",
       " 'completion': 'National Archives'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_lora_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4aa0f6-193f-46fb-8ac3-ce82a67cfb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The Space Race was a 20th-century competition between two Cold War rivals, the Soviet Union (USSR) and the United States (US), for supremacy in spaceflight capability. It had its origins in the missile-based nuclear arms race between the two nations that occurred following World War II, enabled by captured German rocket technology and personnel. The technological superiority required for such supremacy was seen as necessary for national security, and symbolic of ideological superiority. The Space Race spawned pioneering efforts to launch artificial satellites, unmanned space probes of the Moon, Venus, and Mars, and human spaceflight in low Earth orbit and to the Moon. The competition began on August 2, 1955, when the Soviet Union responded to the US announcement four days earlier of intent to launch artificial satellites for the International Geophysical Year, by declaring they would also launch a satellite \"in the near future\". The Soviet Union beat the US to this, with the October 4, 1957 orbiting of Sputnik 1, and later beat the US to the first human in space, Yuri Gagarin, on April 12, 1961. The Space Race peaked with the July 20, 1969 US landing of the first humans on the Moon with Apollo 11. The USSR tried but failed manned lunar missions, and eventually cancelled them and concentrated on Earth orbital space stations. A period of détente followed with the April 1972 agreement on a co-operative Apollo–Soyuz Test Project, resulting in the July 1975 rendezvous in Earth orbit of a US astronaut crew with a Soviet cosmonaut crew.\\nSputnik 1 started orbiting on what date? answer: ',\n",
       " 'completion': 'October 4, 1957'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_lora_val_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6274e-7aaf-466a-ac31-49e193960edb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d61a403-8c49-43d7-8afb-1fe0718a2553",
   "metadata": {},
   "source": [
    "## Write NeMo Customization Data to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936cf42-7de2-460d-a6b2-9926d112f366",
   "metadata": {},
   "source": [
    "We will ultimately upload our p-tuning data to the NeMo Service where it can be used for fine tuning. First we need to write it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a979e9-550c-40ff-89b1-6dac49e1dc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_nemo_train_filename = 'data/squad_nemo_train_prompts_and_answers_1000.jsonl'\n",
    "qa_nemo_val_filename = 'data/squad_nemo_val_prompts_and_answers_200.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4746e9a0-8d4c-4730-b7da-aec2eb6c4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(qa_nemo_train_filename, 'w') as f:\n",
    "    for p_and_a in qa_lora_train_data:\n",
    "        f.write(json.dumps(p_and_a) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "217c03ed-c9bc-4e69-a261-8dadfd89d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(qa_nemo_val_filename, 'w') as f:\n",
    "    for p_and_a in qa_lora_val_data:\n",
    "        f.write(json.dumps(p_and_a) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec3ad8-cd27-4b31-b574-41ee33be9274",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a36016-6719-4481-8ff7-2ad47fa61982",
   "metadata": {},
   "source": [
    "## Upload Data to NeMo Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71659c-7d75-4a14-a199-2fdd512fa9cb",
   "metadata": {},
   "source": [
    "With the data written to file in JSON lines format, we can now upload it to NeMo Service. As we did earlier, we will mock this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db8453ff-c20f-49f6-9b25-03f6ef61555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_response = upload(qa_nemo_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c912b4a-0d1a-453b-bcdd-9446208ac820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f17e25cd-fd08-42b4-a508-12f48985be35',\n",
       " 'name': 'data/squad_nemo_train_prompts_and_answers_1000.jsonl',\n",
       " 'size': 834612,\n",
       " 'number_of_samples': 1000,\n",
       " 'format': 'jsonl',\n",
       " 'usage_category': 'dataset',\n",
       " 'org_id': 'abcdefghijkl',\n",
       " 'user_id': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'ready_at': '0001-01-01T00:00:00Z',\n",
       " 'created_at': '2024-05-29T17:07:41.645836Z'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b38837e-fd74-400a-a089-eaf140373d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_response = upload(qa_nemo_val_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2f121d-964e-47ae-8009-04e45083e53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '30655aa3-17de-41b1-8d73-ddd4a3fadded',\n",
       " 'name': 'data/squad_nemo_val_prompts_and_answers_200.jsonl',\n",
       " 'size': 172165,\n",
       " 'number_of_samples': 200,\n",
       " 'format': 'jsonl',\n",
       " 'usage_category': 'dataset',\n",
       " 'org_id': 'abcdefghijkl',\n",
       " 'user_id': 'abcdefghijklmnopqrstuvwxyz',\n",
       " 'ready_at': '0001-01-01T00:00:00Z',\n",
       " 'created_at': '2024-05-29T17:07:46.835624Z'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a802b9-5b52-43d8-9511-ecaf245338bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910e3da-22ee-44d3-b782-00e4045809ff",
   "metadata": {},
   "source": [
    "## Exercise: LoRA Fine-tune GPT8B for Extractive QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2dc0a-6580-431f-8b95-a5e286f63b33",
   "metadata": {},
   "source": [
    "For this exercise you will perform LoRA fine-tuning on GPT8B with the training and validation data you just wrote to file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601468fe-6194-44bb-a0c6-12a4200469db",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61bdd6-d5f4-44b4-a5c1-b295548ed993",
   "metadata": {},
   "source": [
    "Correctly launch a (mock) LoRA customization using `create_customization` immediately below. On success, when you ascertain the customization ID, set the `customization_id` variable below to it for use later in the notebook.\n",
    "\n",
    "In order to complete this task you'll need to pass `create_customization` the following arguments:\n",
    "- `model`: This should be a LoRA fine-tuneable GPT8B model. You can use the `LoraModels` enum provided above if you wish.\n",
    "- `training_dataset_file_id`: This should be the file ID returned to you above when you (mock) uploaded the training data to NeMo Service.\n",
    "- `validation_dataset_file_id`: This should be the file ID returned to you above when you (mock) uploaded the validation data to NeMo Service.\n",
    "- `adapter_dim`: Use the default value of `32`.\n",
    "- `epochs`: Train for 1 epoch.\n",
    "\n",
    "Worth mentioning is that since we are not providing `validation_data` explicity, NeMo Service will simply use 10% of the training data we provide for validation.\n",
    "\n",
    "If you get stuck, feel free to check out the *Solution* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18d14e05-9e5e-424f-aa13-1be26bbccb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LoRA customization job for GPT8B succesfully launched! Customization ID: ebd552dc-a050-4987-afca-9136d45fbad1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_customization(model=LoraModels.gpt8b.value,\n",
    "                     training_dataset_file_id='f17e25cd-fd08-42b4-a508-12f48985be35',\n",
    "                     validation_dataset_file_id='30655aa3-17de-41b1-8d73-ddd4a3fadded',\n",
    "                     adapter_dim=32,\n",
    "                     epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615209a3-b463-4a98-b518-b260976d62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "customization_id = 'ebd552dc-a050-4987-afca-9136d45fbad1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1504bc87-42ea-4d8b-8b9d-d40b4dffa7f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c19a9e-e829-47c1-821b-6d4725d81217",
   "metadata": {},
   "source": [
    "## Perform Extractive QA with GPT8B LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c4ceb-9d37-4f1f-a01c-bab472598a65",
   "metadata": {},
   "source": [
    "Next we will try the LoRA fine-tuned GPT8B model for the extractive QA task. First we create a model instance, using the LoRA GPT8B base model and providing the model customization ID ascertained from NeMo Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41c598c-97e9-439d-908d-d1f720fcf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt8b_lora = NemoServiceBaseModel(LoraModels.gpt8b.value, customization_id=customization_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80f77b-8de1-4636-b3b6-fb9f45bf5096",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c7fba-82c5-4027-8e44-b7b078142044",
   "metadata": {},
   "source": [
    "Let's try a single QA prompt out on GPT8B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6645878a-c3c9-4b14-8fd0-37251d55f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, label = test_prompts_and_answers[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43dd6b66-cfcb-4880-bcd9-04ced8e827bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Sahara (Arabic: الصحراء الكبرى\\u200e, aṣ-ṣaḥrāʾ al-kubrā\\u202f, 'the Greatest Desert') is the largest hot desert in the world. It is the third largest desert after Antarctica and the Arctic. Its surface area of 9,400,000 square kilometres (3,600,000 sq mi)[citation needed]—including the Libyan Desert—is comparable to the respective land areas of China or the United States. The desert comprises much of the land found within North Africa, excluding the fertile coastal region situated against the Mediterranean Sea, the Atlas Mountains of the Maghreb, and the Nile Valley of Egypt and Sudan. The Sahara stretches from the Red Sea in the east and the Mediterranean in the north, to the Atlantic Ocean in the west, where the landscape gradually transitions to a coastal plain. To the south, it is delimited by the Sahel, a belt of semi-arid tropical savanna around the Niger River valley and Sudan Region of Sub-Saharan Africa. The Sahara can be divided into several regions, including the western Sahara, the central Ahaggar Mountains, the Tibesti Mountains, the Aïr Mountains, the Ténéré desert, and the Libyan Desert. Its name is derived from the plural Arabic language word for desert (صحارى ṣaḥārā\\u202f [ˈsˤɑħɑːrɑː]).\\nWhat is the surface area of the Sahara Desert? answer: \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28e03f92-649d-4104-890c-f3389ad5f0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9,400,000 square kilometres (3,600,000 sq mi)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8382ce-26c4-4e01-bdd7-c03ad0d6938e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9,400,000 square kilometres (3,600,000 sq mi)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt8b_lora.generate(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c23206-3578-440d-bb6a-5e96249a7203",
   "metadata": {},
   "source": [
    "At a glance it looks like the LoRA fine-tuned GPT8B model is doing well. Unlike in the previous notebook where we used just the base GPT8B model, this response does not go on and on, the answer looks to be extracted directly from the text, and is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482c1be-4dec-4dee-9a7e-92aad4c257dd",
   "metadata": {},
   "source": [
    "### Try on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cbfa9-4e77-4f79-9214-4ae77266341b",
   "metadata": {},
   "source": [
    "Now let's try the fine-tuned GPT8B model on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972daa93-9897-4887-908f-2474e44c2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Label: Chekiang\n",
      "\n",
      "Response: 1503\n",
      "Label: 1503\n",
      "\n",
      "Response: 1999, the Detroit River\n",
      "Label: Detroit River\n",
      "\n",
      "Response: 19th century revival of Georgian architecture\n",
      "Label: Colonial Revival\n",
      "\n",
      "Response: 1) volcanic\n",
      "Label: volcanic\n",
      "\n",
      "Response: 1588\n",
      "Label: 1588\n",
      "\n",
      "Response: ‘The Church of Jesus Christ of Latter-day Saints’\n",
      "Label: The Church of Jesus Christ of Latter-day Saints\n",
      "\n",
      "Response: 1,214 km (754 mi) long and considered the longest uninterrupted border within the European Union.\n",
      "Label: Atlantic Ocean\n",
      "\n",
      "Response: 161,785 people resided on Guam\n",
      "Label: United States\n",
      "\n",
      "Response: 1. spheres 2. rods 3. spirals\n",
      "Label: spheres to rods and spirals\n",
      "\n",
      "Response: 9,400,000 square kilometres (3,600,000 sq mi)\n",
      "Label: 9,400,000 square kilometres (3,600,000 sq mi)\n",
      "\n",
      "Response: 55% are non-denominational Muslims\n",
      "Label: (55%) are non-denominational Muslims\n",
      "\n",
      "Response: 1. Roman Catholicism and 2. Eastern Orthodoxy\n",
      "Label: Roman Catholicism and Eastern Orthodoxy\n",
      "\n",
      "Response: furthering a particular social cause or advocating for a particular point of view\n",
      "Label: furthering a particular social cause or advocating for a particular point of view\n",
      "\n",
      "Response: 5000 BC\n",
      "Label: Mexico\n",
      "\n",
      "Response: 1. fiction or non-fiction and 2. poetry or prose\n",
      "Label: poetry or prose\n",
      "\n",
      "Response: 1947\n",
      "Label: 1947\n",
      "\n",
      "Response: 1. Compression\n",
      "Label: Compression\n",
      "\n",
      "Response: 1901\n",
      "Label: the National League\n",
      "\n",
      "Response: 1. Certified Nutrition Specialist or CNS\n",
      "Label: Certified Nutrition Specialist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in test_prompts_and_answers:\n",
    "    response = gpt8b_lora.generate(prompt).strip()\n",
    "    print(f'Response: {response}')\n",
    "    print(f'Label: {answer}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ac838-dd6c-4117-80f7-47ad6bde2cab",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935df50-e097-4fcf-bb0f-4b219019f2d1",
   "metadata": {},
   "source": [
    "The LoRA fine-tuned GPT8B model is not peforming perfectly, however it is doing a relatively good job. At times its answers are incorrect, and it sometimes lists out its responses, but for the most part it is able to perform the task we would like. We will be interested to see how it peforms on the task we intend it for instead of responding to the SQuAD questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
